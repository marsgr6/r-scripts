{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a714c40cb03e442c973ce50a9fa0a598","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"8224641990e7485fbe64a696773a1338"},{"cell_type":"markdown","metadata":{"cell_id":"5146442a2080454b800a5d7d10d52e2c","deepnote_cell_type":"markdown"},"source":"## Week 7 - Activity\n\n### **First task**\n\n- What is optimization **(Bustos)**\n\nLa optimización en inteligencia artificial es un concepto fundamental que desempeña un papel crucial en el ajuste y mejora del rendimiento de modelos de aprendizaje automático, redes neuronales y diversos sistemas de IA. En su núcleo, la **optimización implica el proceso de encontrar la mejor solución posible dentro de un espacio de búsqueda definido**, al tiempo que se adhieren a restricciones u objetivos específicos. Esto se logra a menudo minimizando o maximizando una función objetivo, que podría representar el error en un modelo predictivo, el costo de un proceso de producción u otra métrica mensurable.\nUna de las técnicas de optimización más ampliamente utilizadas en IA es el descenso de gradiente. En el descenso de gradiente, el algoritmo ajusta de manera iterativa los parámetros de un modelo para minimizar la función de coste. Lo hace calculando el gradiente de la función de coste con respecto a los parámetros del modelo y luego actualizando estos parámetros de una manera que mueve el modelo hacia la solución óptima. Este proceso se repite hasta que se alcanza un mínimo satisfactorio de la función de coste. El descenso de gradiente se presenta en varias variantes, incluido el descenso de gradiente estocástico (SGD) y el descenso de gradiente por lotes pequeños, lo que lo hace factible para conjuntos de datos grandes y modelos complejos. Es la piedra angular del entrenamiento de redes neuronales profundas, donde encontrar el conjunto correcto de pesos y sesgos es esencial para lograr una alta precisión en tareas como el reconocimiento de imágenes, el procesamiento de lenguaje natural y más.\nLos [**algoritmos genéticos**](https://github.com/marsgr6/r-scripts/blob/master/notebooks/genetic_algorithms.ipynb) son otra técnica de optimización inspirada en el proceso de selección natural y la evolución. Funcionan creando una población de soluciones potenciales, conocidas como individuos, y mejorando iterativamente esta población a lo largo de varias generaciones. Los individuos que funcionan bien en la resolución del problema tienen más probabilidades de ser seleccionados para producir descendencia, transmitiendo sus rasgos y soluciones a la próxima generación. Con el tiempo, esto imita el proceso de evolución y la población tiende a converger hacia soluciones mejores. Los algoritmos genéticos son especialmente útiles en problemas donde el espacio de búsqueda es grande y complejo, y donde es difícil definir un gradiente como en los métodos de optimización tradicionales. Tienen aplicaciones en diversas áreas, incluyendo la robótica, la optimización de diseño y el desarrollo de estrategias de juego.\nLa **optimización por enjambre** de partículas (PSO) es otro enfoque de optimización inspirado en el comportamiento natural, específicamente en el comportamiento de agrupación o enjambre de aves y peces. En PSO, un grupo de partículas representa soluciones potenciales a un problema. Cada partícula ajusta su posición en el espacio de búsqueda en función de su propia experiencia y la experiencia de las partículas vecinas. Este movimiento colectivo ayuda a las partículas a explorar el espacio de búsqueda y converger hacia soluciones mejores con el tiempo. La optimización por enjambre de partículas es particularmente eficaz en problemas con múltiples objetivos o cuando el proceso de optimización implica encontrar un óptimo global dentro de un paisaje complejo. Tiene aplicaciones en campos como el diseño de ingeniería, la modelización económica y la asignación dinámica de recursos.\nLa optimización en IA va más allá de estas técnicas específicas e incluye una amplia variedad de algoritmos, heurísticas y métodos. Cada enfoque de optimización se elige en función de la naturaleza del problema, la complejidad del espacio de búsqueda y los recursos disponibles. En última instancia, el objetivo es encontrar la mejor solución posible, ya sea en el entrenamiento de modelos de aprendizaje automático, la optimización de procesos empresariales o la resolución de problemas complejos del mundo real. Los algoritmos de optimización son los caballos de batalla de la IA, que nos permiten aprovechar el poder de los datos y la informática para tomar mejores decisiones y mejorar la eficiencia de diversos sistemas y aplicaciones.\n\n  - What is numerical optimization **(Cuenca)**\n\nLa optimización numérica se enfoca en encontrar la mejor solución a un problema, generalmente mediante la minimización o maximización de una función matemática (llamada función objetivo) sujeta a restricciones. Esta búsqueda se realiza mediante algoritmos numéricos que [exploran y evalúan diferentes combinaciones de valores](https://github.com/marsgr6/r-scripts/blob/master/notebooks/genetic_algorithms.ipynb) de variables para encontrar el conjunto de valores que optimiza la función objetivo, cumpliendo con las restricciones establecidas. Los métodos de optimización numérica pueden variar en complejidad y enfoque, pero comparten el objetivo común de encontrar soluciones numéricas óptimas en problemas de decisión, diseño, control y otros dominios.\n\n- What is gradient descent? **(Murminacho)**\n  - What are some applications of gradient descent? **(Naranjo)**\n\n  El descenso de gradiente es un algoritmo esencial en el aprendizaje automático que nos ayuda a encontrar la mejor solución a un problema ajustando gradualmente los parámetros de un modelo. Imagina que quieres ajustar una recta a puntos de datos. El descenso de gradiente te permite encontrar la recta que minimiza la diferencia entre las predicciones y los valores reales. Para hacerlo, el algoritmo calcula la pendiente de la función de error en relación con los parámetros, y luego ajusta los parámetros en la dirección que reduce gradualmente el error. Repite este proceso hasta que la recta se ajuste de la mejor manera posible.\nEste concepto se aplica en una amplia variedad de problemas, desde predecir precios de casas hasta clasificar correos electrónicos como spam o no spam. Puede ser un poco más complejo en aplicaciones de aprendizaje profundo, pero la idea subyacente es la misma: encontrar los mejores ajustes para los parámetros de un modelo mediante un proceso iterativo. A medida que avanzas en el aprendizaje automático, entender y aplicar el descenso de gradiente se convierte en una habilidad esencial.\n\n- How gradient descent works? **(Moreano)**\n  - Check the code in the following [notebook](https://deepnote.com/workspace/mario-gonzalez-911d-e512259f-42a9-4514-8972-e208f19e4b48/project/ai2202210-cce3455d-da08-4c10-a6fe-39f2a30c6a51/notebook/00_06_gradient_descent_202410-b68659540ed34ba98c45cea798e3faf4)\n  - What is the learning reate in gradient descent? **(Rueda)**\n\n- Check in the [MLP code ](https://deepnote.com/workspace/mario-gonzalez-911d-e512259f-42a9-4514-8972-e208f19e4b48/project/ai2202210-cce3455d-da08-4c10-a6fe-39f2a30c6a51/notebook/00_05_mlp_202410-251cd891b5c7492cb7312f59d364c1db) where the gradient descent is present. **(Ruales)**\n\n- Sources:\n  - [Gradient Descent: The Engine of Learning and Optimization](https://medium.com/@HeCanThink/gradient-descent-the-engine-of-learning-and-optimization-cf1f4bd0e3a8)\n  - [How Does the Gradient Descent Algorithm Work in Machine Learning?](https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/#:~:text=Gradient%20descent%20is%20an%20optimization%20algorithm%20used%20to%20minimize%20the,the%20predicted%20and%20actual%20values.)\n\n### **Second task**\n\n- What is supervised learning? **(Salazar)**\n- What is classification? **(Escudero)**\n  - Binary classification **(Escudero)**\n  - Multiclass **(Escudero)**\n- Which machine learning algorithms can be used for supervised learning? **(Ocaña)**\n\n¿Qué algoritmos de aprendizaje automático se pueden utilizar para el aprendizaje supervisado?\nRespuesta: Existen varios algoritmos de aprendizaje automático que se pueden utilizar para tareas de aprendizaje supervisado. Algunos de los más comunes incluyen:\n1. [Regresión Lineal](https://marsgr6.github.io/presentations/curve_fitting_and_interpolation.slides.html#/)\n2. Regresión Logística\n3. Árboles de Decisión\n4. Bosques Aleatorios\n5. Máquinas de Vectores de Soporte (SVM)\n6. K-Vecinos Más Cercanos (KNN)\n7. Naive Bayes\n8. Redes Neuronales (Aprendizaje Profundo)\n9. Algoritmos de Impulso Gradual (por ejemplo, XGBoost, LightGBM)\n10. Análisis Discriminante Lineal (LDA)\nLa elección del algoritmo depende de la naturaleza del problema, el tipo de datos y los objetivos específicos de su tarea de aprendizaje supervisado. Diferentes algoritmos tienen fortalezas y debilidades diferentes, por lo que es importante seleccionar el que sea más adecuado para su problema en particular.\n\n- Sources: \n  - [Taxonomia Ciencia de Datos](https://github.com/marsgr6/r-scripts/blob/master/slides/Taxonomia_CD.pdf)\n","block_group":"756c0eec9dac4a5fa476d8a3ced48864"},{"cell_type":"markdown","metadata":{"deepnote_img_src":"image-20231107-195708.png","cell_id":"233d2476ec194f82a2370a059c1a74ab","deepnote_cell_type":"image"},"source":"<img src=\"image-20231107-195708.png\" width=\"\" align=\"\" />","block_group":"9dd38dec3907413e9707c24daab55b70"},{"cell_type":"markdown","metadata":{"cell_id":"eadb1793ff614a92ab3e4debadefb1fe","deepnote_cell_type":"markdown"},"source":"","block_group":"916d213b11fc46bd9c55c10a31cbf9bd"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cce3455d-da08-4c10-a6fe-39f2a30c6a51' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"d410992e2a78416f9cb5a1fac294f6fe","deepnote_execution_queue":[]}}