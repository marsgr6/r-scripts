{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Data Science?\n",
    "\n",
    "![Data Science Venn Diagram](https://images.squarespace-cdn.com/content/v1/5150aec6e4b0e340ec52710a/1364352051365-HZAS3CLBF7ABLE3F5OBY/Data_Science_VD.png)\n",
    "\n",
    "- **Drew Conway** highlights these three skill: \n",
    "    1. hacking skills, \n",
    "    2. math and stats knowledge, \n",
    "    3. and substantive expertise.\n",
    "<br><br>\n",
    "\n",
    "- First, none is discipline specific, but more importantly, each of these skills are on their own very valuable, but when combined with only one other are at best simply not data science, or at worst downright dangerous.\n",
    "- Data is a commodity traded electronically; therefore, in order to be in this market **you need to speak hacker**. Being able to manipulate text files at the command-line, understanding vectorized operations, thinking algorithmically; these are the hacking skills that make for a successful data hacker.\n",
    "- Once you have acquired and cleaned the data, the next step is to actually **extract insight** from it. In order to do this, you need to **apply appropriate math and statistics methods**, which requires at least a baseline familiarity with these tools. This is not to say that a PhD in statistics is required to be a competent data scientist, but it does require knowing what an ordinary least squares regression is and how to interpret it.\n",
    "- **Data plus math and statistics only gets you machine learning**, which is great if that is what you are interested in, but not if you are doing data science. Science is about discovery and building knowledge, which requires some motivating questions about the world and hypotheses that can be brought to data and tested with statistical methods. \n",
    "- **Substantive expertise plus math and statistics** knowledge is where most **traditional research falls**.\n",
    "- Finally, a word on the **hacking skills plus substantive expertise danger zone**. This is where I place people who, \"know enough to be dangerous,\" and is the most problematic area of the diagram. In this area people who are perfectly capable of extracting and structuring data, likely related to a field they know quite a bit about, and probably even know enough R to run a linear regression and report the coefficients; but they lack any understanding of what those coefficients mean. It is from this part of the diagram that the phrase \"lies, damned lies, and statistics\" emanates, because either through ignorance or malice this overlap of skills gives people the ability to create what appears to be a legitimate analysis without any understanding of how they got there or what they have created. Fortunately, it requires near willful ignorance to acquire hacking skills and substantive expertise without also learning some math and statistics along the way. As such, the danger zone is sparsely populated, however, it does not take many to produce a lot of damage.\n",
    "\n",
    "__[Source](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)__.\n",
    "\n",
    "## Course Scope\n",
    "\n",
    "The Course __[Introduction to Python 3 for data analytics](https://marsgr6.github.io/presentations/Python3_Course_Content.html#/)__ aims to provide hacking skills by using the Python language (you should also __[explore R](https://www.r-project.org/about.html)__). Although the course will not address the mathematical/statistic part, a short introduction of the topics' intuitions will be given, as well as, resources and recommended bibliography to deepen on this topics.\n",
    "\n",
    "- We will cover the Extract-**Transform**-Load process, but we will focus on the Transform step.\n",
    "\n",
    "- Once you have tidy data, we will perform an **Exploratory Data Analysis** in order to uncover the structure of our data and summarize their characteristics mainly using visual methods. We want to get a detailed picture of the data we are dealing with.\n",
    "\n",
    "- Finally, we will build **models** from our data in order to estimate the relationships among variables and to predict events. We will compare data and test hypotheses and identify trends and relationships that provide real predictive value to aid decisions making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "- In data sience much of the time is spent sourcing, cleaning and storing the raw data required for the more insightful upstream analysis. It is said that in Data Mininig 80% is preprocessing and 20% mining. \n",
    "\n",
    "- Little time is actually spent implementing algorithms from scratch. Indeed, most statistical tools come with their inner workings wrapped up in neat R packages and Python modules.\n",
    "\n",
    "- The 'extract-transform-load' (ETL) process is critical to the success of any data science team. Larger organizations will have dedicated data engineers to meet their complex data infrastructure requirements, but younger companies will often depend upon their data scientists to possess strong, all-round data engineering skills of their own.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*KH_F6lZnyt71IDi7uiP3HA.png)\n",
    "\n",
    "### Think of an ETL process you have carried out.\n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "- It is often said that 80% of data analysis is spent on the cleaning and preparing data.\n",
    "\n",
    "- Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy\n",
    "\n",
    "- Like families, tidy datasets are all alike but every messy dataset is messy in its own way. \n",
    "\n",
    "- Tidy data is a standard way of mapping the meaning of a dataset to its structure. \n",
    "- A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n",
    "\n",
    "    1. Each variable forms a column.\n",
    "    2. Each observation forms a row.\n",
    "    3. Each type of observational unit forms a table.\n",
    "    \n",
    "__[Source](https://medium.freecodecamp.org/aspiring-data-scientist-master-these-fundamentals-be7c54350868)__.\n",
    "    \n",
    "### Let us try an __[example](https://nbviewer.org/github/marsgr6/r-scripts/blob/master/notebooks/python_intro_ds/pandas_python.ipynb)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory DatExploratory Data Analysis (EDA)\n",
    "\n",
    "We want to generate questions about our data, search for answers by visualising, transforming, and modelling; and then refine our questions and/or generate new questions. We perform this process iteratively using EDA.\n",
    "\n",
    "EDA is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to\n",
    "maximize insight into a data set in order to:\n",
    "- uncover underlying structure;\n",
    "- extract important variables;\n",
    "- detect outliers and anomalies;\n",
    "- test underlying assumptions;\n",
    "- develop parsimonious models; and\n",
    "- determine optimal factor settings.\n",
    "\n",
    "A nice introductory reading about EDA can be found in __[R for Data Science: Chapter 3. Exploratory Data Analysis](http://r4ds.had.co.nz/exploratory-data-analysis.html)__ \n",
    "\n",
    "Let's try som Exploratory Data Analysis (EDA) with __[Seaborn](https://nbviewer.org/github/marsgr6/r-scripts/blob/master/notebooks/python_intro_ds/seaborn.ipynb)__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
